import os
import shutil
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
from PIL import Image, UnidentifiedImageError
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split

# === CONFIG ===
IMG_SIZE = (260, 260)
BATCH_SIZE = 64
EPOCHS = 20
MODEL_PATH = "best_model.keras"
ORIG_DATASET_PATH = r"C:/Users/auste/Desktop/dataset"
CLEAN_SPLIT_PATH = r"C:/Users/auste/Desktop/dataset_split"

# === STRATIFIED SPLIT FUNCTION ===
def stratified_split_dataset(original_path, split_path, test_size=0.15, val_size=0.15):
    if os.path.exists(split_path):
        print("Stratified dataset already prepared.")
        return

    os.makedirs(split_path, exist_ok=True)
    all_images = []
    labels = []

    for label in os.listdir(original_path):
        label_path = os.path.join(original_path, label)
        if not os.path.isdir(label_path):
            continue
        for fname in os.listdir(label_path):
            if fname.lower().endswith(('.jpg', '.jpeg', '.png')):
                all_images.append(os.path.join(label_path, fname))
                labels.append(label)

    X_temp, X_test, y_temp, y_test = train_test_split(all_images, labels, test_size=test_size, stratify=labels, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_size/(1 - test_size), stratify=y_temp, random_state=42)

    for subset, data, labels_set in zip(['train', 'val', 'test'], [X_train, X_val, X_test], [y_train, y_val, y_test]):
        for path, label in zip(data, labels_set):
            dest_dir = os.path.join(split_path, subset, label)
            os.makedirs(dest_dir, exist_ok=True)
            shutil.copy2(path, os.path.join(dest_dir, os.path.basename(path)))

# === DATA CLEANING ===
def clean_images(dataset_path):
    removed_count = 0
    for root, _, files in os.walk(dataset_path):
        for file in files:
            if not file.lower().endswith(('.png', '.jpg', '.jpeg')):
                continue
            file_path = os.path.join(root, file)
            try:
                with Image.open(file_path) as img:
                    img.verify()
                    with Image.open(file_path) as img:
                        if img.mode != 'RGB':
                            img.convert('RGB').save(file_path)
                        if min(img.size) < 64:
                            os.remove(file_path)
                            removed_count += 1
            except (IOError, SyntaxError, ValueError, UnidentifiedImageError):
                try:
                    os.remove(file_path)
                    removed_count += 1
                except:
                    pass
    return removed_count

# === DATA GENERATORS ===
def create_generators():
    datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        zoom_range=0.2,
        width_shift_range=0.2,
        height_shift_range=0.2,
        horizontal_flip=True,
        vertical_flip=True,
    )

    train_generator = datagen.flow_from_directory(
        os.path.join(CLEAN_SPLIT_PATH, 'train'),
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=True,
        seed=42
    )

    val_generator = datagen.flow_from_directory(
        os.path.join(CLEAN_SPLIT_PATH, 'val'),
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=True,
        seed=42
    )

    test_datagen = ImageDataGenerator(rescale=1./255)
    test_generator = test_datagen.flow_from_directory(
        os.path.join(CLEAN_SPLIT_PATH, 'test'),
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=False,
        classes=train_generator.class_indices
    )

    # Class weight calculation
    counts = np.bincount(train_generator.classes)
    total = sum(counts)
    class_weights = {i: total / (len(counts) * c) for i, c in enumerate(counts)}

    return train_generator, val_generator, test_generator, class_weights

# === MODEL DEFINITION ===
def build_model(num_classes):
    base_model = EfficientNetB0(
        include_top=False,
        weights='imagenet',
        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)
    )
    base_model.trainable = True  # Unfreeze all layers

    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dropout(0.5),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(num_classes, activation='softmax')
    ])

    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# === TRAIN MODEL ===
def train_model(model, train_gen, val_gen, class_weights):
    cb = [
        callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
        callbacks.ModelCheckpoint(MODEL_PATH, monitor='val_loss', save_best_only=True),
        callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)
    ]
    history = model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=EPOCHS,
        class_weight=class_weights,
        callbacks=cb
    )
    return history

# === EVALUATE & VISUALIZE ===
def evaluate_and_plot(model, test_gen):
    y_pred = model.predict(test_gen)
    y_true = test_gen.classes
    y_pred_classes = np.argmax(y_pred, axis=1)
    class_names = list(test_gen.class_indices.keys())

    cm = confusion_matrix(y_true, y_pred_classes)
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

    print("Classification Report:")
    print(classification_report(y_true, y_pred_classes, target_names=class_names))

    # Show predictions
    plt.figure(figsize=(15, 10))
    images, labels = next(test_gen)
    for i in range(9):
        plt.subplot(3, 3, i+1)
        plt.imshow(images[i])
        true_label = class_names[np.argmax(labels[i])]
        pred_label = class_names[np.argmax(y_pred[i])]
        color = 'green' if true_label == pred_label else 'red'
        plt.title(f"True: {true_label}\nPred: {pred_label}", color=color)
        plt.axis('off')
    plt.tight_layout()
    plt.show()

# === MAIN ===
def main():
    print("Stratifying and cleaning dataset...")
    stratified_split_dataset(ORIG_DATASET_PATH, CLEAN_SPLIT_PATH)
    removed = clean_images(CLEAN_SPLIT_PATH)
    print(f"Removed {removed} problematic images")

    print("Preparing data...")
    train_gen, val_gen, test_gen, class_weights = create_generators()
    num_classes = len(train_gen.class_indices)

    print("Building model...")
    model = build_model(num_classes)

    print("Training model...")
    history = train_model(model, train_gen, val_gen, class_weights)

    print("Loading best model for evaluation...")
    model = load_model(MODEL_PATH)

    print("Evaluating on test data...")
    evaluate_and_plot(model, test_gen)

if __name__ == "__main__":
    main()

